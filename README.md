# transformer-from-scratch
Implementation of Attention Is All You Need (https://arxiv.org/abs/1706.03762)

Potential Literature Survey papers:
1) A Survey of Deep Learning Techniques for Neural Machine Translation (https://arxiv.org/pdf/2002.07526.pdf)
2) Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473)
3) Sequence to Sequence Learning with Neural Networks (https://arxiv.org/abs/1409.3215)
4) Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (https://arxiv.org/abs/1406.1078)
5) Long Short-Term Memory-Networks for Machine Reading (https://arxiv.org/abs/1601.06733)
6) A Structured Self-attentive Sentence Embedding (https://arxiv.org/abs/1703.03130)
7) Layer Normalization (https://arxiv.org/abs/1607.06450)

Other papers:
1) Findings of the 2021 Conference on Machine Translation (WMT21) (https://aclanthology.org/2021.wmt-1.1.pdf)
2) Neural Machine Translation of Rare Words with Subword Units (https://arxiv.org/abs/1508.07909)
3) Literature Survey: Neural Machine Translation (https://www.cfilt.iitb.ac.in/resources/surveys/NMT-survey-paper.pdf)
4) The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation (https://arxiv.org/abs/1804.09849)
6) A Survey of Multilingual Neural Machine Translation (https://dl.acm.org/doi/pdf/10.1145/3406095)
7) Survey: Machine Translation for Indian Language (https://www.ripublication.com/ijaer18/ijaerv13n1_29.pdf)
8) Beyond English-Centric Multilingual Machine Translation (https://www.jmlr.org/papers/volume22/20-1307/20-1307.pdf)
