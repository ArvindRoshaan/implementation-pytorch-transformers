# transformer-from-scratch
Implementation of Transformer architecture proposed in the Attention Is All You Need (https://arxiv.org/abs/1706.03762) paper

Contributors:
1) Sanyog Sharma - AI21MTECH12003
2) Arvind Roshaan - AI21MTECH12004
3) Billakurthi Shai Sasi Deep - SM21MTECH12006
4) HN Srikanth - SM21MTECH12012
5) Deevanshu Gupta - SM21MTECH12014

Literature Survey papers:
1) Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (https://arxiv.org/abs/1406.1078) by Billakurthi Shai Sasi Deep
2) Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473) by Sanyog Sharma
3) A Structured Self-attentive Sentence Embedding (https://arxiv.org/abs/1703.03130) by Deevanshu Gupta
4) Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (https://arxiv.org/abs/1609.08144) by Billakurthi Shai Sasi Deep
5) Massive Exploration of Neural Machine Translation Architectures (https://arxiv.org/abs/1703.03906) by Billakurthi Shai Sasi Deep
6) Sequence to Sequence Learning with Neural Networks (https://arxiv.org/abs/1409.3215) by Deevanshu Gupta
7) Attention is all you need by (https://arxiv.org/abs/1706.03762) Srikanth

Results Replicated from Literature: done by Sanyog and Srikanth

Code Implemenation from Scratch: By Arvind Roshaan

